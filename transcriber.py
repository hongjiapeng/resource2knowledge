# -*- coding: utf-8 -*-
"""
ğŸ™ Whisper Transcriber Module
ä½¿ç”¨ faster-whisper è¿›è¡Œæœ¬åœ°è¯­éŸ³è½¬æ–‡æœ¬
"""

import gc
import torch
from typing import Optional, Dict
from faster_whisper import WhisperModel


class WhisperTranscriber:
    """æœ¬åœ° Whisper è½¬å½•å™¨"""
    
    # æ¨¡å‹é…ç½® - small æ¨¡å‹é€‚åˆ 8GB æ˜¾å­˜
    MODEL_SIZE = "small"
    COMPUTE_TYPE = "float16"  # float16 åœ¨ RTX 5060 ä¸Šçº¦å ç”¨ 2GB
    
    def __init__(self, model_path: Optional[str] = None):
        """
        åˆå§‹åŒ–è½¬å½•å™¨
        
        Args:
            model_path: è‡ªå®šä¹‰æ¨¡å‹è·¯å¾„ (å¯é€‰)
        """
        self.model = None
        self.model_path = model_path
    
    def load_model(self, device: str = "cuda"):
        """
        åŠ è½½ Whisper æ¨¡å‹åˆ° GPU
        
        Args:
            device: è¿è¡Œè®¾å¤‡ ("cuda" æˆ– "cpu")
        """
        if self.model is not None:
            print("âœ… æ¨¡å‹å·²åŠ è½½")
            return
        
        print(f"ğŸ“¥ åŠ è½½ Whisper {self.MODEL_SIZE} æ¨¡å‹...")
        print(f"ğŸ–¥ï¸ è®¾å¤‡: {device}")
        print(f"ğŸ“Š è®¡ç®—ç±»å‹: {self.COMPUTE_TYPE}")
        
        try:
            self.model = WhisperModel(
                self.MODEL_SIZE,
                device=device,
                compute_type=self.COMPUTE_TYPE,
                download_root=self.model_path
            )
            print("âœ… æ¨¡å‹åŠ è½½å®Œæˆ")
            
            # æ‰“å°æ˜¾å­˜å ç”¨é¢„ä¼°
            self._print_vram_usage()
            
        except Exception as e:
            print(f"âŒ æ¨¡å‹åŠ è½½å¤±è´¥: {e}")
            raise
    
    def _print_vram_usage(self):
        """æ‰“å°æ˜¾å­˜å ç”¨é¢„ä¼°"""
        if torch.cuda.is_available():
            allocated = torch.cuda.memory_allocated() / 1024**3
            reserved = torch.cuda.memory_reserved() / 1024**3
            print(f"ğŸ“Š æ˜¾å­˜å ç”¨: {allocated:.2f}GB (å·²åˆ†é…) / {reserved:.2f}GB (å·²é¢„ç•™)")
    
    def transcribe(
        self,
        audio_path: str,
        language: Optional[str] = None,
        task: str = "transcribe"
    ) -> Dict:
        """
        è½¬å½•éŸ³é¢‘æ–‡ä»¶
        
        Args:
            audio_path: éŸ³é¢‘æ–‡ä»¶è·¯å¾„
            language: è¯­è¨€ä»£ç  (None = è‡ªåŠ¨æ£€æµ‹)
            task: "transcribe" æˆ– "translate"
            
        Returns:
            åŒ…å«æ–‡æœ¬ã€æ®µè½ã€è¯­è¨€ä¿¡æ¯çš„å­—å…¸
        """
        if self.model is None:
            self.load_model()
        
        print(f"ğŸ™ï¸ å¼€å§‹è½¬å½•: {audio_path}")
        
        # è‡ªåŠ¨æ£€æµ‹è¯­è¨€ (é»˜è®¤ä¼˜å…ˆä¸­æ–‡)
        if language is None:
            language = "zh"  # é»˜è®¤ä¸­æ–‡
        
        try:
            # æ‰§è¡Œè½¬å½•
            segments, info = self.model.transcribe(
                audio_path,
                language=language,
                task=task,
                beam_size=5,
                vad_filter=True,  # è¯­éŸ³æ´»åŠ¨æ£€æµ‹
                vad_parameters=dict(min_silence_duration_ms=500)
            )
            
            # æ”¶é›†æ‰€æœ‰æ®µè½
            transcript_segments = []
            full_text = []
            
            print("ğŸ“ è½¬å½•è¿›åº¦:")
            for segment in segments:
                text = segment.text.strip()
                transcript_segments.append({
                    'start': segment.start,
                    'end': segment.end,
                    'text': text
                })
                full_text.append(text)
                print(f"  [{segment.start:.1f}s - {segment.end:.1f}s] {text}")
            
            result = {
                'text': ' '.join(full_text),
                'segments': transcript_segments,
                'language': info.language if hasattr(info, 'language') else language,
                'duration': info.duration if hasattr(info, 'duration') else 0,
            }
            
            print(f"âœ… è½¬å½•å®Œæˆ! æ—¶é•¿: {result['duration']:.1f}ç§’")
            print(f"ğŸ“Š æ–‡æœ¬é•¿åº¦: {len(result['text'])} å­—ç¬¦")
            
            return result
            
        except Exception as e:
            raise Exception(f"è½¬å½•å¤±è´¥: {str(e)}")
    
    def unload_model(self):
        """å¸è½½æ¨¡å‹å¹¶é‡Šæ”¾æ˜¾å­˜"""
        if self.model is not None:
            print("ğŸ”„ é‡Šæ”¾ Whisper æ¨¡å‹...")
            del self.model
            self.model = None
            
            # å¼ºåˆ¶åƒåœ¾å›æ”¶
            gc.collect()
            
            # æ¸…ç©º CUDA ç¼“å­˜
            if torch.cuda.is_available():
                torch.cuda.empty_cache()
                torch.cuda.synchronize()
            
            print("âœ… æ˜¾å­˜å·²é‡Šæ”¾")
            self._print_vram_usage()
    
    def __enter__(self):
        """ä¸Šä¸‹æ–‡ç®¡ç†å™¨å…¥å£"""
        self.load_model()
        return self
    
    def __exit__(self, exc_type, exc_val, exc_tb):
        """ä¸Šä¸‹æ–‡ç®¡ç†å™¨å‡ºå£ - è‡ªåŠ¨é‡Šæ”¾æ˜¾å­˜"""
        self.unload_model()


def get_vram_requirement(model_size: str = "small") -> Dict:
    """
    è·å–å„æ¨¡å‹çš„æ˜¾å­˜éœ€æ±‚é¢„ä¼°
    
    Returns:
        æ˜¾å­˜éœ€æ±‚å­—å…¸
    """
    requirements = {
        "tiny": {"vram": "~1GB", "speed": "æœ€å¿«", "accuracy": "æœ€ä½"},
        "base": {"vram": "~1GB", "speed": "å¿«", "accuracy": "åŸºç¡€"},
        "small": {"vram": "~2GB", "speed": "ä¸­ç­‰", "accuracy": "è‰¯å¥½"},
        "medium": {"vram": "~5GB", "speed": "è¾ƒæ…¢", "accuracy": "å¾ˆå¥½"},
        "large": {"vram": "~10GB", "speed": "æ…¢", "accuracy": "æœ€ä½³"},
    }
    return requirements.get(model_size, requirements["small"])


if __name__ == "__main__":
    # æµ‹è¯•è½¬å½•
    import sys
    
    if len(sys.argv) < 2:
        print("ç”¨æ³•: python transcriber.py <audio_file>")
        sys.exit(1)
    
    audio_file = sys.argv[1]
    
    with WhisperTranscriber() as transcriber:
        result = transcriber.transcribe(audio_file)
        print("\n=== è½¬å½•ç»“æœ ===")
        print(result['text'][:500] + "..." if len(result['text']) > 500 else result['text'])
