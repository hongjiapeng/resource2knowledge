# -*- coding: utf-8 -*-
"""
ğŸ§  Summarizer Module
ä½¿ç”¨ Ollama æœ¬åœ° LLM ç”Ÿæˆæ‘˜è¦
"""

import gc
import torch
import ollama
from typing import Optional, Dict, List


class Summarizer:
    """æœ¬åœ° LLM æ‘˜è¦ç”Ÿæˆå™¨"""
    
    # æ¨èæ¨¡å‹é…ç½® (é€‚åˆ 8GB æ˜¾å­˜)
    DEFAULT_MODEL = "qwen2.5:7b-instruct-q4_K_M"
    
    # å¤‡é€‰æ¨¡å‹
    ALT_MODELS = {
        "qwen2.5:7b-instruct-q4_K_M": {"vram": "~4-5GB", "speed": "ä¸­ç­‰", "quality": "ä¼˜ç§€"},
        "llama3.2:3b-instruct-q4_K_M": {"vram": "~2-3GB", "speed": "å¿«", "quality": "è‰¯å¥½"},
        "phi3.5:3.8b-mini-instruct-q4_K_M": {"vram": "~2GB", "speed": "å¾ˆå¿«", "quality": "ä¸€èˆ¬"},
        "mistral:7b-instruct-q4_K_M": {"vram": "~4GB", "speed": "ä¸­ç­‰", "quality": "è‰¯å¥½"},
    }
    
    SYSTEM_PROMPT = """ä½ æ˜¯ä¸€ä¸ªä¸“ä¸šçš„è§†é¢‘å†…å®¹åˆ†æå¸ˆã€‚ä½ çš„ä»»åŠ¡æ˜¯å¯¹è§†é¢‘ transcriptï¼ˆè½¬å½•æ–‡æœ¬ï¼‰è¿›è¡Œæ€»ç»“ã€‚

è¯·æŒ‰ä»¥ä¸‹æ ¼å¼è¾“å‡º JSONï¼š
{
    "summary": "è§†é¢‘å†…å®¹çš„è¯¦ç»†æ€»ç»“ (100-500å­—)",
    "key_points": ["è¦ç‚¹1", "è¦ç‚¹2", "è¦ç‚¹3", "è¦ç‚¹4", "è¦ç‚¹5"],
    "tags": ["æ ‡ç­¾1", "æ ‡ç­¾2", "æ ‡ç­¾3"],
    "category": "è§†é¢‘åˆ†ç±»",
    "sentiment": "positive/negative/neutral",
    "language": "zh/en/mixed"
}

è¦æ±‚ï¼š
- summary éœ€è¦è¦†ç›–è§†é¢‘çš„æ ¸å¿ƒå†…å®¹å’Œç»“è®º
- key_points æå–æœ€é‡è¦çš„ 5 ä¸ªè¦ç‚¹
- tags åŸºäºå†…å®¹è‡ªåŠ¨ç”Ÿæˆç›¸å…³æ ‡ç­¾
- category ä½¿ç”¨ç®€çŸ­çš„ä¸­æ–‡åˆ†ç±»
- ç›´æ¥è¾“å‡º JSONï¼Œä¸è¦å…¶ä»–å†…å®¹"""

    # å›¾æ–‡å†…å®¹åˆ†æ prompt
    IMAGE_TEXT_PROMPT = """ä½ æ˜¯ä¸€ä¸ªä¸“ä¸šçš„å°çº¢ä¹¦å†…å®¹åˆ†æå¸ˆã€‚ä½ çš„ä»»åŠ¡æ˜¯å¯¹å›¾æ–‡ç¬”è®°å†…å®¹è¿›è¡Œåˆ†ææ€»ç»“ã€‚

è¯·æŒ‰ä»¥ä¸‹æ ¼å¼è¾“å‡º JSONï¼š
{
    "summary": "å†…å®¹çš„è¯¦ç»†æ€»ç»“ (100-500å­—)",
    "key_points": ["è¦ç‚¹1", "è¦ç‚¹2", "è¦ç‚¹3", "è¦ç‚¹4", "è¦ç‚¹5"],
    "tags": ["æ ‡ç­¾1", "æ ‡ç­¾2", "æ ‡ç­¾3"],
    "category": "å†…å®¹åˆ†ç±»",
    "sentiment": "positive/negative/neutral",
    "language": "zh/en/mixed"
}

è¦æ±‚ï¼š
- summary éœ€è¦è¦†ç›–å›¾æ–‡çš„æ ¸å¿ƒå†…å®¹å’Œä½œè€…è§‚ç‚¹
- key_points æå–æœ€é‡è¦çš„ 5 ä¸ªè¦ç‚¹
- tags åŸºäºå†…å®¹è‡ªåŠ¨ç”Ÿæˆç›¸å…³æ ‡ç­¾
- category ä½¿ç”¨ç®€çŸ­çš„ä¸­æ–‡åˆ†ç±»
- ç›´æ¥è¾“å‡º JSONï¼Œä¸è¦å…¶ä»–å†…å®¹"""

    def __init__(self, model: Optional[str] = None):
        """
        åˆå§‹åŒ–æ‘˜è¦ç”Ÿæˆå™¨
        
        Args:
            model: Ollama æ¨¡å‹åç§°
        """
        self.model = model or self.DEFAULT_MODEL
    
    def check_ollama(self) -> bool:
        """æ£€æŸ¥ Ollama æœåŠ¡æ˜¯å¦å¯ç”¨"""
        try:
            ollama.list()
            return True
        except Exception as e:
            print(f"âŒ Ollama è¿æ¥å¤±è´¥: {e}")
            print("ğŸ’¡ è¯·ç¡®ä¿ Ollama å·²å¯åŠ¨: ollama serve")
            return False
    
    def check_model_loaded(self) -> bool:
        """æ£€æŸ¥æ¨¡å‹æ˜¯å¦å·²ä¸‹è½½"""
        try:
            models = ollama.list()
            model_names = [m.get('name', '') for m in models.get('models', [])]
            
            print(f"ğŸ“‹ å·²å®‰è£…æ¨¡å‹: {model_names}")
            
            # å¤„ç†æ¨¡å‹åç§°æ ¼å¼ - æ›´å®½æ¾çš„åŒ¹é…
            base_model = self.model.split(':')[0]
            for name in model_names:
                if base_model.lower() in name.lower():
                    return True
            
            print(f"âš ï¸ æ¨¡å‹ {self.model} æœªåœ¨åˆ—è¡¨ä¸­")
            print(f"ğŸ“¥ è¯·è¿è¡Œ: ollama pull {self.model}")
            return False
        except Exception as e:
            print(f"âŒ æ£€æŸ¥æ¨¡å‹å¤±è´¥: {e}")
            return False
    
    def load_model(self):
        """é¢„çƒ­æ¨¡å‹ (å¯é€‰)"""
        print(f"ğŸ”¥ é¢„çƒ­æ¨¡å‹: {self.model}")
        try:
            # ç®€å•çš„é¢„çƒ­è¯·æ±‚
            ollama.generate(
                model=self.model,
                prompt="ä½ å¥½",
                options={"num_predict": 1}
            )
            print("âœ… æ¨¡å‹é¢„çƒ­å®Œæˆ")
        except Exception as e:
            print(f"âš ï¸ é¢„çƒ­å¤±è´¥: {e}")
        
        # æ‰“å°æ˜¾å­˜å ç”¨
        if torch.cuda.is_available():
            allocated = torch.cuda.memory_allocated() / 1024**3
            print(f"ğŸ“Š æ˜¾å­˜å ç”¨: {allocated:.2f}GB")
    
    def summarize(self, transcript: str, max_length: int = 2000, content_type: str = 'video') -> Dict:
        """
        ç”Ÿæˆæ‘˜è¦
        
        Args:
            transcript: è½¬å½•æ–‡æœ¬
            max_length: æœ€å¤§è¾“å…¥é•¿åº¦ (å­—ç¬¦)
            content_type: å†…å®¹ç±»å‹ ('video' æˆ– 'image_text')
            
        Returns:
            åŒ…å« summary, key_points, tags ç­‰çš„å­—å…¸
        """
        # æˆªæ–­è¿‡é•¿çš„æ–‡æœ¬
        if len(transcript) > max_length:
            print(f"ğŸ“„ æ–‡æœ¬è¿‡é•¿ ({len(transcript)} å­—ç¬¦)ï¼Œæˆªæ–­è‡³ {max_length} å­—ç¬¦")
            transcript = transcript[:max_length] + "..."
        
        # é€‰æ‹©åˆé€‚çš„ prompt
        if content_type == 'image_text':
            system_prompt = self.IMAGE_TEXT_PROMPT
            content_label = "å›¾æ–‡ç¬”è®°å†…å®¹"
        else:
            system_prompt = self.SYSTEM_PROMPT
            content_label = "è§†é¢‘è½¬å½•æ–‡æœ¬"
        
        print(f"ğŸ§  å¼€å§‹ç”Ÿæˆæ‘˜è¦ (æ¨¡å‹: {self.model}, ç±»å‹: {content_type})")
        
        try:
            response = ollama.generate(
                model=self.model,
                prompt=f"{system_prompt}\n\nä»¥ä¸‹æ˜¯{content_label}:\n\n{transcript}",
                format="json",
                options={
                    "temperature": 0.3,  # ä½æ¸©åº¦ï¼Œæ›´ç¡®å®šæ€§çš„è¾“å‡º
                    "num_predict": 1000,
                }
            )
            
            import json
            result = json.loads(response.response)
            
            print("âœ… æ‘˜è¦ç”Ÿæˆå®Œæˆ")
            return {
                'summary': result.get('summary', ''),
                'key_points': result.get('key_points', []),
                'tags': result.get('tags', []),
                'category': result.get('category', 'æœªåˆ†ç±»'),
                'sentiment': result.get('sentiment', 'neutral'),
                'language': result.get('language', 'zh'),
            }
            
        except json.JSONDecodeError as e:
            print(f"âš ï¸ JSON è§£æå¤±è´¥ï¼Œä½¿ç”¨å¤‡ç”¨æ–¹æ³•")
            return self._fallback_summarize(transcript)
        except Exception as e:
            raise Exception(f"æ‘˜è¦ç”Ÿæˆå¤±è´¥: {str(e)}")
    
    def _fallback_summarize(self, transcript: str) -> Dict:
        """å¤‡ç”¨æ‘˜è¦æ–¹æ³• (å½“ JSON è§£æå¤±è´¥æ—¶)"""
        print("ğŸ”„ ä½¿ç”¨å¤‡ç”¨æ‘˜è¦æ–¹æ³•...")
        
        try:
            response = ollama.generate(
                model=self.model,
                prompt=f"è¯·ç”¨ä¸­æ–‡æ€»ç»“ä»¥ä¸‹è§†é¢‘è½¬å½•å†…å®¹ï¼Œæå–3-5ä¸ªè¦ç‚¹:\n\n{transcript[:1500]}",
                options={"temperature": 0.3, "num_predict": 500}
            )
            
            return {
                'summary': response.response,
                'key_points': [],
                'tags': [],
                'category': 'æœªåˆ†ç±»',
                'sentiment': 'neutral',
                'language': 'zh',
            }
        except Exception as e:
            raise Exception(f"å¤‡ç”¨æ‘˜è¦ä¹Ÿå¤±è´¥: {str(e)}")
    
    def unload_model(self):
        """é‡Šæ”¾æ˜¾å­˜ (é€šè¿‡æ¸…ç©ºç¼“å­˜)"""
        gc.collect()
        if torch.cuda.is_available():
            torch.cuda.empty_cache()
            torch.cuda.synchronize()
        print("âœ… LLM æ˜¾å­˜å·²é‡Šæ”¾")
    
    @staticmethod
    def get_available_models() -> List[Dict]:
        """è·å–å¯ç”¨çš„ Ollama æ¨¡å‹åˆ—è¡¨"""
        try:
            result = ollama.list()
            return result.get('models', [])
        except:
            return []


def estimate_vram(model: str) -> str:
    """é¢„ä¼°æ¨¡å‹æ˜¾å­˜å ç”¨"""
    configs = Summarizer.ALT_MODELS
    return configs.get(model, {}).get('vram', 'æœªçŸ¥')


if __name__ == "__main__":
    # æµ‹è¯•
    summarizer = Summarizer()
    
    if not summarizer.check_ollama():
        print("âŒ Ollama æœªè¿è¡Œ")
        exit(1)
    
    if not summarizer.check_model_loaded():
        exit(1)
    
    # æµ‹è¯•æ‘˜è¦
    test_transcript = """
    ä»Šå¤©æˆ‘ä»¬æ¥èŠèŠå¦‚ä½•å…¥é—¨æœºå™¨å­¦ä¹ ã€‚æœºå™¨å­¦ä¹ æ˜¯äººå·¥æ™ºèƒ½çš„ä¸€ä¸ªé‡è¦åˆ†æ”¯ï¼Œ
    å®ƒè®©è®¡ç®—æœºèƒ½å¤Ÿä»æ•°æ®ä¸­å­¦ä¹ ï¼Œè€Œä¸éœ€è¦æ˜ç¡®çš„ç¼–ç¨‹æŒ‡ä»¤ã€‚
    é¦–å…ˆï¼Œæˆ‘ä»¬éœ€è¦äº†è§£åŸºæœ¬æ¦‚å¿µï¼šç›‘ç£å­¦ä¹ ã€æ— ç›‘ç£å­¦ä¹ å’Œå¼ºåŒ–å­¦ä¹ ã€‚
    ç›‘ç£å­¦ä¹ æ˜¯æœ€å¸¸è§çš„æ–¹å¼ï¼Œæ¯”å¦‚åˆ†ç±»å’Œå›å½’é—®é¢˜ã€‚
    æ— ç›‘ç£å­¦ä¹ ç”¨äºèšç±»å’Œé™ç»´ï¼Œå¼ºåŒ–å­¦ä¹ åˆ™ç”¨äºæ¸¸æˆå’Œæœºå™¨äººæ§åˆ¶ã€‚
    æ¨èåˆå­¦è€…ä» Python åŸºç¡€å¼€å§‹ï¼Œå­¦ä¹  NumPyã€Pandas ç­‰åº“ï¼Œ
    ç„¶åé€æ­¥å­¦ä¹  scikit-learnï¼Œæœ€åæ·±å…¥æ·±åº¦å­¦ä¹ æ¡†æ¶ã€‚
    """
    
    result = summarizer.summarize(test_transcript)
    import json
    print(json.dumps(result, indent=2, ensure_ascii=False))
